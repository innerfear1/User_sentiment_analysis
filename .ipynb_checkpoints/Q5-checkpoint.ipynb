{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "04e6995f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "import os\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from wordcloud import WordCloud\n",
    "from os import path\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "9c0bbefd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>订餐   半小时 不见 餐   催 挂 电话</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>太 不靠 谱           漏发 订单 菜 送错   早 送 小时</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>超级 超级 喜欢 吃   每次 好开心   口味   建议 手套 戴 几双   吃 完 形象...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>谢谢 快递 哥哥   准时   辛苦</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>差得 餐厅   上菜 慢   贵   餐厅 饿死 那种   面条 味道   一碗      ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17948</th>\n",
       "      <td>0</td>\n",
       "      <td>感觉 好像 发挥 稳定   几此   有好有坏   吃 日料 关注 三文鱼   中等   不...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17949</th>\n",
       "      <td>1</td>\n",
       "      <td>订 九点 送到   十二点</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17950</th>\n",
       "      <td>1</td>\n",
       "      <td>差评   辣白菜 五花肉 石锅 拌 饭 超级 难吃   五花肉 硬 难吃   感觉 新鲜  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17951</th>\n",
       "      <td>1</td>\n",
       "      <td>贵 好吃   老婆 几次 非要   一吃 真 难吃   蒸 没错   炒菜 油腻   菜 好...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17952</th>\n",
       "      <td>1</td>\n",
       "      <td>小炒 肉太辣   辣椒 放太多   凉粉 更辣   筷子 要用 手 抓 吃</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17953 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       target                                            comment\n",
       "0           1                             订餐   半小时 不见 餐   催 挂 电话\n",
       "1           1               太 不靠 谱           漏发 订单 菜 送错   早 送 小时\n",
       "2           0  超级 超级 喜欢 吃   每次 好开心   口味   建议 手套 戴 几双   吃 完 形象...\n",
       "3           0                                 谢谢 快递 哥哥   准时   辛苦\n",
       "4           1  差得 餐厅   上菜 慢   贵   餐厅 饿死 那种   面条 味道   一碗      ...\n",
       "...       ...                                                ...\n",
       "17948       0  感觉 好像 发挥 稳定   几此   有好有坏   吃 日料 关注 三文鱼   中等   不...\n",
       "17949       1                                    订 九点 送到   十二点  \n",
       "17950       1  差评   辣白菜 五花肉 石锅 拌 饭 超级 难吃   五花肉 硬 难吃   感觉 新鲜  ...\n",
       "17951       1  贵 好吃   老婆 几次 非要   一吃 真 难吃   蒸 没错   炒菜 油腻   菜 好...\n",
       "17952       1              小炒 肉太辣   辣椒 放太多   凉粉 更辣   筷子 要用 手 抓 吃\n",
       "\n",
       "[17953 rows x 2 columns]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q5_data = pd.read_excel('train.xlsx')\n",
    "Q5_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "78f5b620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      label                                                cut\n",
      "0         1                             订餐   半小时 不见 餐   催 挂 电话\n",
      "1         1               太 不靠 谱           漏发 订单 菜 送错   早 送 小时\n",
      "2         0  超级 超级 喜欢 吃   每次 好开心   口味   建议 手套 戴 几双   吃 完 形象...\n",
      "3         0                                 谢谢 快递 哥哥   准时   辛苦\n",
      "4         1  差得 餐厅   上菜 慢   贵   餐厅 饿死 那种   面条 味道   一碗      ...\n",
      "...     ...                                                ...\n",
      "17948     0  感觉 好像 发挥 稳定   几此   有好有坏   吃 日料 关注 三文鱼   中等   不...\n",
      "17949     1                                    订 九点 送到   十二点  \n",
      "17950     1  差评   辣白菜 五花肉 石锅 拌 饭 超级 难吃   五花肉 硬 难吃   感觉 新鲜  ...\n",
      "17951     1  贵 好吃   老婆 几次 非要   一吃 真 难吃   蒸 没错   炒菜 油腻   菜 好...\n",
      "17952     1              小炒 肉太辣   辣椒 放太多   凉粉 更辣   筷子 要用 手 抓 吃\n",
      "\n",
      "[17953 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "raw_df = pd.read_excel('train.xlsx')\n",
    "stop_words = set(open('ChineseStopWords.txt', encoding='utf-8').read().strip().split('\\n'))\n",
    " \n",
    "# 分词，生成新的dataframe\n",
    "data = []  # 存储分词后的数据和label\n",
    "for comment,target in raw_df.values:\n",
    "    words = [\n",
    "        # 过滤停用词\n",
    "        word for word in jieba.cut(str(comment)) if word.strip() and word.strip() not in stop_words\n",
    "    ]\n",
    "    data.append([' '.join(words), target])\n",
    " \n",
    "df = pd.DataFrame(data, columns=['label', 'cut'])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "986cbdb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse._csr.csr_matrix'>\n",
      "['龙鱼 味道', '龟苓膏', '龟苓膏 味道', '龟苓膏 总体', '龟苓膏 特别', '龟速', '龟速 下次', '龟速 十二点', '龟龄膏', '龟龄膏 正宗']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 2))  \n",
    "X = vectorizer.fit_transform(df['cut'].astype(str).values)  \n",
    "print(type(X))  \n",
    "X.shape  \n",
    "# 打印最后10个特征词\n",
    "print(vectorizer.get_feature_names()[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "8d0b8be2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['一个半', '一个半 小时', '不好', '不错', '不错 价格', '不错 喜欢', '两个 小时', '他家', '价格',\n",
       "       '再也', '再也不会', '分钟', '味道', '味道 不错', '喜欢', '垃圾', '太差', '太慢', '失望',\n",
       "       '好吃', '好喝', '实惠', '小哥', '小时', '小时 送到', '差差', '差评', '很差', '很快',\n",
       "       '恶心', '打电话', '挺不错', '朋友', '每次', '没法', '热情', '环境', '环境 不错', '电话',\n",
       "       '真的', '米饭', '美味', '订单', '辛苦', '还会', '送到', '送来', '适合', '难吃', '面包'],\n",
       "      dtype='<U11')"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "selector = SelectKBest(chi2, k=50)\n",
    "new_X = selector.fit_transform(X, df['label'])\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "np.array(vectorizer.get_feature_names())[selector.get_support()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "5ee5cd66",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 38)\t1.0\n",
      "  (1, 42)\t0.8246362152349382\n",
      "  (1, 23)\t0.5656634268918193\n",
      "  (2, 33)\t0.7930202251233012\n",
      "  (2, 14)\t0.6091953073895011\n",
      "  (3, 43)\t1.0\n",
      "  (4, 12)\t1.0\n",
      "  (5, 33)\t1.0\n",
      "  (6, 39)\t0.35414520220151335\n",
      "  (6, 30)\t0.5758258711206533\n",
      "  (6, 23)\t0.42688805004738406\n",
      "  (6, 6)\t0.6006432673663562\n",
      "  (10, 21)\t0.6707918241606595\n",
      "  (10, 13)\t0.5349074204897583\n",
      "  (10, 12)\t0.360447904100054\n",
      "  (10, 3)\t0.36604601974900086\n",
      "  (11, 2)\t1.0\n",
      "  (16, 12)\t1.0\n",
      "  (18, 14)\t0.5936373413129162\n",
      "  (18, 7)\t0.8047326928856141\n",
      "  (20, 39)\t0.8981255383593754\n",
      "  (20, 35)\t0.39847073600517696\n",
      "  (20, 12)\t0.18598814449898388\n",
      "  (21, 19)\t1.0\n",
      "  (22, 28)\t0.8602977758230352\n",
      "  :\t:\n",
      "  (17932, 11)\t1.0\n",
      "  (17933, 36)\t1.0\n",
      "  (17934, 15)\t0.7021838865021887\n",
      "  (17934, 0)\t0.7119956387062223\n",
      "  (17935, 36)\t0.8055726966861207\n",
      "  (17935, 12)\t0.5924969454384145\n",
      "  (17936, 48)\t1.0\n",
      "  (17937, 14)\t1.0\n",
      "  (17938, 39)\t0.9049925030438183\n",
      "  (17938, 33)\t0.42542751372529325\n",
      "  (17941, 23)\t0.47881398637719963\n",
      "  (17941, 17)\t0.5956901030695095\n",
      "  (17941, 11)\t0.6448957028501677\n",
      "  (17942, 23)\t1.0\n",
      "  (17944, 49)\t0.9045644664083063\n",
      "  (17944, 3)\t0.42633686928467246\n",
      "  (17945, 12)\t1.0\n",
      "  (17946, 3)\t1.0\n",
      "  (17947, 19)\t1.0\n",
      "  (17949, 45)\t1.0\n",
      "  (17950, 48)\t0.7152909749265751\n",
      "  (17950, 26)\t0.47311903454445037\n",
      "  (17950, 10)\t0.5143123567836152\n",
      "  (17951, 48)\t0.5816131898402487\n",
      "  (17951, 19)\t0.8134654863015707\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "weight_X = TfidfTransformer().fit_transform(new_X)\n",
    "print(weight_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5a737b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'weight_X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_3184\\4067899509.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtrain_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'label'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.3\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# 训练集测试集7 3开\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'train data: '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_X\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# 打印训练集shape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'test data: '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_X\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# 打印测试集shape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'weight_X' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_X, test_X, train_y, test_y = train_test_split(weight_X, df['label'], test_size=0.3)  # 训练集测试集7 3开\n",
    "\n",
    "print('train data: ', train_X.shape, train_y.shape)  # 打印训练集shape\n",
    "print('test data: ', test_X.shape, test_y.shape)  # 打印测试集shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2981c405",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用简单神经网络模型\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "mlp = MLPClassifier()\n",
    "mlp.fit(train_X,train_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "d348268c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用逻辑回归模型\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression().fit(train_X, train_y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "f939d728",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.779428147047902"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(test_X, test_y)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "904ef0c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc:  0.7853107344632768\n",
      "test acc:  0.7799851466765689\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(C=10, max_iter=1000).fit(train_X, train_y)  # 声明某些参数\n",
    "print('train acc: ', clf.score(train_X, train_y))  # 评估训练集acc\n",
    "print('test acc: ', clf.score(test_X, test_y))  # 评估训练集acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "f6bf79a4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>userId</th>\n",
       "      <th>sellerId</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>72916</td>\n",
       "      <td>11976</td>\n",
       "      <td>2020-10-01 12:38:17</td>\n",
       "      <td>text：买的人很多，味道好，是十年以上的老店了~~~~\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>39091</td>\n",
       "      <td>34918</td>\n",
       "      <td>2020-10-01 17:35:04</td>\n",
       "      <td>原来是辣的流泪，所以叫伤心凉粉。厉害，味道不错，不过吃完还想吃。\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>289933</td>\n",
       "      <td>59229</td>\n",
       "      <td>2020-10-01 21:00:17</td>\n",
       "      <td>text：外卖好快的，适合中午点</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>77764</td>\n",
       "      <td>27100</td>\n",
       "      <td>2020-10-02 06:40:07</td>\n",
       "      <td>text：整体感觉就是一般，味道也一般，可能是个人因素哈．\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>24687</td>\n",
       "      <td>30939</td>\n",
       "      <td>2020-10-02 07:28:33</td>\n",
       "      <td>text：我不喜欢</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1495</th>\n",
       "      <td>NaN</td>\n",
       "      <td>279254</td>\n",
       "      <td>54944</td>\n",
       "      <td>2021-08-01 08:15:56</td>\n",
       "      <td>text：感觉价钱太贵了。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1496</th>\n",
       "      <td>NaN</td>\n",
       "      <td>18389</td>\n",
       "      <td>59255</td>\n",
       "      <td>2021-08-01 08:41:58</td>\n",
       "      <td>text：鸭血粉丝不正宗，鸭血不嫩汤也没味道，桂花凉粉还凑合\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1497</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1901</td>\n",
       "      <td>31488</td>\n",
       "      <td>2021-07-31 14:23:52</td>\n",
       "      <td>text：外卖天天有红包\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1498</th>\n",
       "      <td>NaN</td>\n",
       "      <td>542</td>\n",
       "      <td>58869</td>\n",
       "      <td>2021-07-31 17:06:09</td>\n",
       "      <td>text：适合羊毛党，折扣多</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1499</th>\n",
       "      <td>NaN</td>\n",
       "      <td>14491</td>\n",
       "      <td>29376</td>\n",
       "      <td>2021-07-29 01:11:08</td>\n",
       "      <td>text：名不副实，差评必须给\\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1500 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      target  userId  sellerId            timestamp  \\\n",
       "0        NaN   72916     11976  2020-10-01 12:38:17   \n",
       "1        NaN   39091     34918  2020-10-01 17:35:04   \n",
       "2        NaN  289933     59229  2020-10-01 21:00:17   \n",
       "3        NaN   77764     27100  2020-10-02 06:40:07   \n",
       "4        NaN   24687     30939  2020-10-02 07:28:33   \n",
       "...      ...     ...       ...                  ...   \n",
       "1495     NaN  279254     54944  2021-08-01 08:15:56   \n",
       "1496     NaN   18389     59255  2021-08-01 08:41:58   \n",
       "1497     NaN    1901     31488  2021-07-31 14:23:52   \n",
       "1498     NaN     542     58869  2021-07-31 17:06:09   \n",
       "1499     NaN   14491     29376  2021-07-29 01:11:08   \n",
       "\n",
       "                                 comment  \n",
       "0         text：买的人很多，味道好，是十年以上的老店了~~~~\\n  \n",
       "1     原来是辣的流泪，所以叫伤心凉粉。厉害，味道不错，不过吃完还想吃。\\n  \n",
       "2                       text：外卖好快的，适合中午点  \n",
       "3        text：整体感觉就是一般，味道也一般，可能是个人因素哈．\\n  \n",
       "4                              text：我不喜欢  \n",
       "...                                  ...  \n",
       "1495                       text：感觉价钱太贵了。  \n",
       "1496    text：鸭血粉丝不正宗，鸭血不嫩汤也没味道，桂花凉粉还凑合\\n  \n",
       "1497                      text：外卖天天有红包\\n  \n",
       "1498                      text：适合羊毛党，折扣多  \n",
       "1499                   text：名不副实，差评必须给\\n  \n",
       "\n",
       "[1500 rows x 5 columns]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "import re\n",
    "pickle.dump(clf, open('./Q5/lr.model', 'wb'))\n",
    "data = pd.read_excel('./Q5/test.xlsx')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "df973be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(x):\n",
    "    text = re.sub(r\"[A-Za-z0-9(),!?\\'\\'\\.【】⊙﹏…ฅω*ฅ❀༵〜⚘;%^:#＆@？！‘’：_，“”、-。~～·\\.-×☆]\",\" \",x)\n",
    "    return text.strip()\n",
    "data['comment']=data['comment'].apply(lambda x:clean_data(x).replace('\\n',''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "d6fe0bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword=[line.strip() for line in open('ChineseStopWords.txt','r',encoding=\"utf-8\").readlines()]\n",
    "def cut_word(text):\n",
    "    try:\n",
    "        clean_text=[]\n",
    "        for i in jieba.lcut(text):\n",
    "            if i:\n",
    "                if i not in stopword and i:\n",
    "                    clean_text.append(i)\n",
    "        return ''.join(clean_text)\n",
    "    except:\n",
    "        print(text)\n",
    "data['comment']=data['comment'].apply(lambda x:cut_word(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "8ae1ce26",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_excel('./Q5/clean_test.xlsx',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "5522fa0a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'CountVectorizer' object has no attribute 'isnull'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_6092\\3234800640.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mvectorizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mngram_range\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtestX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'comment'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mtestX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misnull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'CountVectorizer' object has no attribute 'isnull'"
     ]
    }
   ],
   "source": [
    "data = pd.read_excel('./Q5/clean_test.xlsx')\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 2))  \n",
    "testX = vectorizer.fit(data['comment'].astype(str).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "3f5ae0b6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got scalar array instead:\narray=CountVectorizer(ngram_range=(1, 2)).\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_6092\\3509159229.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# 加载模型并预测\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mclf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./Q5/lr.model'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'target'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\anaconda\\lib\\site-packages\\sklearn\\linear_model\\_base.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    423\u001b[0m             \u001b[0mVector\u001b[0m \u001b[0mcontaining\u001b[0m \u001b[0mthe\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0meach\u001b[0m \u001b[0msample\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    424\u001b[0m         \"\"\"\n\u001b[1;32m--> 425\u001b[1;33m         \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    426\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    427\u001b[0m             \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\anaconda\\lib\\site-packages\\sklearn\\linear_model\\_base.py\u001b[0m in \u001b[0;36mdecision_function\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    405\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    406\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 407\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"csr\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    408\u001b[0m         \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msafe_sparse_dot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdense_output\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mintercept_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    409\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mscores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mscores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mscores\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\anaconda\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    564\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Validation should be done on X, y or both.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    565\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 566\u001b[1;33m             \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    567\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    568\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    759\u001b[0m             \u001b[1;31m# If input is scalar raise error\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    760\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 761\u001b[1;33m                 raise ValueError(\n\u001b[0m\u001b[0;32m    762\u001b[0m                     \u001b[1;34m\"Expected 2D array, got scalar array instead:\\narray={}.\\n\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    763\u001b[0m                     \u001b[1;34m\"Reshape your data either using array.reshape(-1, 1) if \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Expected 2D array, got scalar array instead:\narray=CountVectorizer(ngram_range=(1, 2)).\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "# 加载模型并预测\n",
    "clf = pickle.load(open('./Q5/lr.model', 'rb'))\n",
    "data['target'] = clf.predict(testX)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72cb3478",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_excle('./Q5/pretest.xlsx',index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
